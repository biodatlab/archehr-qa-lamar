{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import config\n",
        "from data_processing.loader import load_data, load_xml_as_dataframe\n",
        "from generation.prompt_generator import (\n",
        "    generate_prompt_baseline,\n",
        "    generate_prompt_fewshot,\n",
        "    generate_prompt_rag,\n",
        "    generate_prompt_rag_summary,\n",
        "    generate_prompt_rag_synthetic_cases\n",
        ")\n",
        "from generation.answers_generator import generate_answers\n",
        "from generation.synthetic_data_generator import (\n",
        "    generate_synthetic_answers,\n",
        "    generate_reasoning,\n",
        "    summarize_articles,\n",
        "    generate_synthetic_case\n",
        ")\n",
        "from postprocess import prepare_submission_file\n",
        "from rag.retrieval import build_faiss_index, embed_text, retrieve_articles, load_faiss_index\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "import json\n",
        "import pickle"
      ],
      "metadata": {
        "id": "QyLLF0Wa3bdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure the generative AI model\n",
        "client = config.get_client()"
      ],
      "metadata": {
        "id": "ejrHnVdo3fix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Selection\n",
        "BASELINE_MODEL = \"google/gemini-2.5-pro\"\n",
        "FEWSHOT_MODEL = \"google/gemini-2.0-flash\"\n",
        "RAG_MODEL = \"google/gemini-2.5-pro\"\n",
        "SUMMARIZATION_MODEL = \"google/gemini-2.5-pro\""
      ],
      "metadata": {
        "id": "0jO6zxj33jsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "result = load_data(config.XML_FILE)\n",
        "ground_truth = load_data(config.JSON_KEY_FILE)"
      ],
      "metadata": {
        "id": "9Ewz05P13m6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0. Exemplars and retreival preparation"
      ],
      "metadata": {
        "id": "xV6WKehP8IKa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 0.1. Exemplars synthesis"
      ],
      "metadata": {
        "id": "6BMpRnB85F__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SYNTHETIC_ANSWER_MODEL = \"google/gemini-2.5-pro\"\n",
        "REASONING_MODEL = \"google/gemini-2.5-pro\""
      ],
      "metadata": {
        "id": "aFW6ZUqt5Fen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_synthetic_answers(\n",
        "    client=client,\n",
        "    model_name=SYNTHETIC_ANSWER_MODEL,\n",
        "    result=result,\n",
        "    ground_truth=ground_truth,\n",
        "    output_path=\"data/syn_answer.pkl\"\n",
        ")"
      ],
      "metadata": {
        "id": "Xlb5xH7A5TZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_reasoning(\n",
        "    client=client,\n",
        "    model_name=REASONING_MODEL,\n",
        "    input_path=\"data/syn_answer.pkl\",\n",
        "    output_path=\"data/syn_answer_with_reasoning.pkl\"\n",
        ")"
      ],
      "metadata": {
        "id": "PbOolEqz5Vpz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 0.2. Summarize articles"
      ],
      "metadata": {
        "id": "wdwjx7gu6Mhg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SUMMARIZATION_MODEL_ARTICLES = \"google/gemini-2.5-pro\""
      ],
      "metadata": {
        "id": "IpoD7i9L5nXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summarize_articles(\n",
        "    client=client,\n",
        "    model_name=SUMMARIZATION_MODEL_ARTICLES,\n",
        "    retrieved_articles=retrieved_articles,\n",
        "    output_path=config.SUMMARIZED_ARTICLES_FILE\n",
        ")"
      ],
      "metadata": {
        "id": "Yc7ntAN36gmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 0.3. Generate synthetic cases"
      ],
      "metadata": {
        "id": "EBFKxFMU6kC4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SYNTHETIC_CASE_MODEL = \"google/gemini-2.5-pro\""
      ],
      "metadata": {
        "id": "33bYpbN56jvF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_synthetic_case(\n",
        "    client=client,\n",
        "    model_name=SYNTHETIC_CASE_MODEL,\n",
        "    retrieved_articles=retrieved_articles,\n",
        "    output_path=config.SYNTHETIC_CASES_FILE\n",
        ")"
      ],
      "metadata": {
        "id": "B4FH1D726pCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Baseline experiment"
      ],
      "metadata": {
        "id": "ydKzJKVI3u_X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_answers = generate_answers(\n",
        "    cases=result[\"cases\"],\n",
        "    client=client,\n",
        "    model_name=BASELINE_MODEL,\n",
        "    prompt_fn=generate_prompt_baseline\n",
        ")\n",
        "prepare_submission_file(\n",
        "    baseline_answers,\n",
        "    f\"{config.OUTPUT_DIR}/submission_baseline.json\",\n",
        "    result,\n",
        "    client,\n",
        "    SUMMARIZATION_MODEL\n",
        ")"
      ],
      "metadata": {
        "id": "5HlInEVS3m4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Few-shot experiments"
      ],
      "metadata": {
        "id": "HbpO0Q2P34Vg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_pickle(config.EXEMPLARS_FILE)\n",
        "all_cases = df.to_dict(orient=\"records\")\n",
        "with open('prompts/example.json', 'r') as f:\n",
        "    basic_examples = json.load(f)"
      ],
      "metadata": {
        "id": "QHYkMPfY3m1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.1. Few-shot: basic"
      ],
      "metadata": {
        "id": "ROYfn84l3_MR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fewshot1_answers = generate_answers(\n",
        "    cases=result[\"cases\"],\n",
        "    client=client,\n",
        "    model_name=FEWSHOT_MODEL,\n",
        "    prompt_fn=lambda case: generate_prompt_fewshot(case, basic_examples),\n",
        "    max_retries=5\n",
        ")\n",
        "prepare_submission_file(\n",
        "    fewshot1_answers,\n",
        "    f\"{config.OUTPUT_DIR}/submission_fewshot1.json\",\n",
        "    result,\n",
        "    client,\n",
        "    SUMMARIZATION_MODEL\n",
        ")"
      ],
      "metadata": {
        "id": "NWHRdmHs3m0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.2. Few-shot: LLM-generated exemplars"
      ],
      "metadata": {
        "id": "kbgnX0t04GoB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fewshot2_answers = generate_answers(\n",
        "    cases=result[\"cases\"],\n",
        "    client=client,\n",
        "    model_name=FEWSHOT_MODEL,\n",
        "    prompt_fn=lambda case: generate_prompt_fewshot(case, all_cases),\n",
        "    max_retries=5\n",
        ")\n",
        "prepare_submission_file(\n",
        "    fewshot2_answers,\n",
        "    f\"{config.OUTPUT_DIR}/submission_fewshot2.json\",\n",
        "    result,\n",
        "    client,\n",
        "    SUMMARIZATION_MODEL\n",
        ")"
      ],
      "metadata": {
        "id": "D6dpk2xb3mvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.3. Few-shot: exemplars with reasoning"
      ],
      "metadata": {
        "id": "EGGWiwFP4N_V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fewshot3_answers = generate_answers(\n",
        "    cases=result[\"cases\"],\n",
        "    client=client,\n",
        "    model_name=FEWSHOT_MODEL,\n",
        "    prompt_fn=lambda case: generate_prompt_fewshot(case, all_cases, add_reasoning=True),\n",
        "    max_retries=5\n",
        ")\n",
        "prepare_submission_file(\n",
        "    fewshot3_answers,\n",
        "    f\"{config.OUTPUT_DIR}/submission_fewshot3.json\",\n",
        "    result,\n",
        "    client,\n",
        "    SUMMARIZATION_MODEL\n",
        ")"
      ],
      "metadata": {
        "id": "NxzMmqAP3mr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. RAG experiments"
      ],
      "metadata": {
        "id": "P3nS1AiZ4V0j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_article = pd.read_csv(config.ARTICLE_FILE)\n",
        "df_query = load_xml_as_dataframe(config.XML_FILE)\n",
        "\n",
        "# Load model for RAG\n",
        "query_model = AutoModel.from_pretrained(config.EMBEDDING_MODEL)\n",
        "query_tokenizer = AutoTokenizer.from_pretrained(config.EMBEDDING_MODEL)\n",
        "\n",
        "# Create embeddings and build index\n",
        "question_embeddings = embed_text(df_query['Clinician Question'].tolist(), query_model, query_tokenizer)\n",
        "index = load_faiss_index(config.VECTOR_DATABASE_FILE)\n",
        "\n",
        "# Retrieve articles for all queries\n",
        "retrieved_articles = retrieve_articles(np.array(question_embeddings), index, df_article)\n"
      ],
      "metadata": {
        "id": "touG9Qnc3mZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.1. RAG: full articles"
      ],
      "metadata": {
        "id": "3hcWtcLK4xHM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rag1_answers = generate_answers(\n",
        "    cases=result[\"cases\"],\n",
        "    client=client,\n",
        "    model_name=RAG_MODEL,\n",
        "    prompt_fn=lambda case, idx: generate_prompt_rag(case, retrieved_articles[idx]),\n",
        "    max_retries=5,\n",
        "    use_index=True,\n",
        ")\n",
        "prepare_submission_file(\n",
        "    rag1_answers,\n",
        "    f\"{config.OUTPUT_DIR}/submission_gemini_rag1.json\",\n",
        "    result,\n",
        "    client,\n",
        "    SUMMARIZATION_MODEL\n",
        ")"
      ],
      "metadata": {
        "id": "siU6QNGm4wbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.2. RAG: article summaries"
      ],
      "metadata": {
        "id": "bX6n_mKp6---"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(config.SUMMARIZED_ARTICLES_FILE, 'rb') as f:\n",
        "    summarized_articles = pickle.load(f)\n",
        "with open('prompts/example.json', 'r') as f:\n",
        "    example = json.load(f)[0]"
      ],
      "metadata": {
        "id": "FqYR0soY7CER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag2_answers = generate_answers(\n",
        "    cases=result[\"cases\"],\n",
        "    client=client,\n",
        "    model_name=RAG_MODEL,\n",
        "    prompt_fn=lambda case, idx: generate_prompt_rag_summary(case, summarized_articles[idx], example),\n",
        "    max_retries=5,\n",
        "    use_index=True,\n",
        ")\n",
        "prepare_submission_file(\n",
        "    rag2_answers,\n",
        "    f\"{config.OUTPUT_DIR}/submission_rag_summary.json\",\n",
        "    result,\n",
        "    client,\n",
        "    SUMMARIZATION_MODEL\n",
        ")"
      ],
      "metadata": {
        "id": "NNnbRFJs7GKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.3. RAG: synthetic Cases"
      ],
      "metadata": {
        "id": "Tu6Fh5Ym7RRp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_synthetic = pd.read_csv(config.SYNTHETIC_CASES_FILE)\n",
        "all_synthetic_cases = df_synthetic.to_dict(orient=\"records\")"
      ],
      "metadata": {
        "id": "Dr27exv27Nb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag3_answers = generate_answers(\n",
        "    cases=result[\"cases\"],\n",
        "    client=client,\n",
        "    model_name=RAG_MODEL,\n",
        "    prompt_fn=lambda case: generate_prompt_rag_synthetic_cases(case, all_synthetic_cases),\n",
        "    max_retries=5\n",
        ")\n",
        "prepare_submission_file(\n",
        "    rag3_answers,\n",
        "    f\"{config.OUTPUT_DIR}/submission_rag_synthetic_cases.json\",\n",
        "    result,\n",
        "    client,\n",
        "    SUMMARIZATION_MODEL\n",
        ")\n"
      ],
      "metadata": {
        "id": "DcEH9o7F7PYE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}